{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd981b-f75c-4b71-9f5c-3dd09ca89dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import kdeplot\n",
    "\n",
    "\n",
    "mapping = np.load(\"Data/mapping.npy\")\n",
    "freq = np.load('Data/frequencies.npy')\n",
    "subtraction_signals = np.load('Data/Processed/subtraction_signals.npy')\n",
    "shifted_signals = np.load(\"Data/Processed/shifted_signals.npy\")\n",
    "shifted_residual = np.load(\"Data/Processed/shifted_residual.npy\")\n",
    "\n",
    "data_info = pd.read_csv('Data/data_info.csv')  # Provided\n",
    "velocity_info = pd.read_csv('Data/velocity_info.csv')  # Calculated\n",
    "velocity_info = velocity_info.dropna(subset=[\"Automatic velocity\"]).reset_index(drop=True)\n",
    "n_lines = pd.read_csv(\"Data/Processed/almagal_nlines_all.csv\")\n",
    "\n",
    "CO_idx = np.argmin(np.abs(freq - 220398.42455376702))\n",
    "\n",
    "# Load and re-order labels such that largest cluster is cluster 0 and smalles is cluster -1\n",
    "labels_10 = np.load(\"Data/Processed/labels10.npy\")\n",
    "labels_23 = np.load(\"Data/Processed/labels23.npy\")\n",
    "\n",
    "labels_mask_10 = labels_10 != -1  # Mask for clustered signals\n",
    "labels_mask_23 = labels_23 != -1  # Mask for clustered signals\n",
    "\n",
    "ordered_labels_10 = labels_10.copy()\n",
    "ordered_labels_23 = labels_23.copy()\n",
    "\n",
    "unique_labels, sizes = np.unique(labels_10[labels_mask_10], return_counts=True)\n",
    "for i, label in enumerate(unique_labels[np.argsort(sizes)[::-1]]):\n",
    "    ordered_labels_10[labels_10 == label] = i\n",
    "    \n",
    "unique_labels, sizes = np.unique(labels_23[labels_mask_23], return_counts=True)\n",
    "for i, label in enumerate(unique_labels[np.argsort(sizes)[::-1]]):\n",
    "    ordered_labels_23[labels_23 == label] = i\n",
    "\n",
    "# Match data from tables to data from available sources\n",
    "sc_pair = []\n",
    "data_index_list = []\n",
    "v_index_list = []\n",
    "nlines_index_list = []\n",
    "\n",
    "# Filter data from data_info not in velocity_info\n",
    "velocity_info = velocity_info.dropna(subset=[\"Automatic velocity\"]).reset_index(drop=True)\n",
    "for i in range(len(data_info)):\n",
    "    source = data_info['CLUMP'].iloc[i]\n",
    "    core = data_info['ID'].iloc[i]\n",
    "    \n",
    "    v_index = velocity_info[(velocity_info[\"Source\"] == source) * (velocity_info[\"Core\"] == core)].index.to_list()\n",
    "    nlines_index = n_lines[(n_lines[\"Region\"] == source) * (n_lines[\"Core\"] == core)].index.to_list()\n",
    "    \n",
    "    if len(v_index) > 0 and len(n_lines) > 0:\n",
    "        if len(v_index) > 1:\n",
    "            print(f\"v_index: {v_index}\")\n",
    "        if len(nlines_index) > 1:\n",
    "            print(f\"n_lines: {nlines_index}\")\n",
    "            \n",
    "        v_index_list.append(v_index[0])\n",
    "        nlines_index_list.append(nlines_index[0])\n",
    "        data_index_list.append(i)\n",
    "        \n",
    "        sc_pair.append((source, core))\n",
    "\n",
    "\n",
    "velocity_info = velocity_info.iloc[v_index_list]\n",
    "n_lines = n_lines.iloc[nlines_index_list]\n",
    "data_info = data_info.iloc[data_index_list]\n",
    "\n",
    "assert np.all(velocity_info[\"Source\"].values == n_lines[\"Region\"].values)\n",
    "assert np.all(velocity_info[\"Source\"].values == data_info[\"CLUMP\"].values)\n",
    "assert np.all(velocity_info[\"Core\"].values == n_lines[\"Core\"].values)\n",
    "assert np.all(velocity_info[\"Core\"].values == data_info[\"ID\"].values)\n",
    "\n",
    "ordered_labels_10 = ordered_labels_10[v_index_list]\n",
    "ordered_labels_23 = ordered_labels_23[v_index_list]\n",
    "\n",
    "subtraction_signals = subtraction_signals[v_index_list]\n",
    "subtraction_signals /= np.max(subtraction_signals, axis=1, keepdims=True)\n",
    "\n",
    "# Delete data from H_II regions\n",
    "h2_mask = data_info['RADIO_MATCH'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"Tclump\", \"Llump/Mclump\", \"Surfd_nd\", \"n(H2)\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
