{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71febff9",
   "metadata": {},
   "source": [
    "# Pre-Processing: from raw data to model inputs\n",
    "\n",
    "This notebook will showcase how the different pre-processing steps were performed. This document assumes that there is a folder named \"Data\" with a sub-folder \"Raw\" where the \"sources\" folder that was provided by ICE-CSIC exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c087d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0131a",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "The data will be extracted using the functions under src.data.data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf3ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|██████████| 1017/1017 [18:03<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.data.data_loader import extract_signals\n",
    "from src.data.data_processor import load_data\n",
    "\n",
    "# Define the base directory\n",
    "base_directory = 'Data/Raw/sources'  # Change to where the tar files are located\n",
    "\n",
    "# Extract the sources - uncomment the next line if you haven't done it yet\n",
    "# extract_signals(base_directory)  # Generates a sources_extracted directory under 'Data/Raw'\n",
    "\n",
    "# Store the data as a dictionary\n",
    "data_dict = load_data('Data/Raw/sources_extracted')  # Change to where the extracted files are located"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd512090",
   "metadata": {},
   "source": [
    "## Interpolate data\n",
    "\n",
    "Once the data has been correctly loaded, it must be interpolated to a common base so that all signals share the same channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1cad026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating data: 100%|██████████| 641/641 [00:08<00:00, 78.18it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.data.data_processor import interpolate_data, build_array\n",
    "\n",
    "# Interpolate the data to a common base\n",
    "interpolated_data_dict = interpolate_data(data_dict)\n",
    "\n",
    "# Store the interpolated data as a numpy array for easier use\n",
    "raw_data_spw0, mapping_0 = build_array(interpolated_data_dict, category=\"Intensity\", spw=0)\n",
    "raw_data_spw1, mapping_1 = build_array(interpolated_data_dict, category=\"Intensity\", spw=1)\n",
    "\n",
    "if np.all(np.array(mapping_0)[:, :-1] == np.array(mapping_1)[:, :-1]):\n",
    "    mapping = np.array(mapping_0)[:, :-1]\n",
    "    raw_data = np.concatenate((raw_data_spw0, raw_data_spw1), axis=1)\n",
    "else:\n",
    "    raise ValueError(\"Mappings for intensity's spw0 and spw1 do not match.\")\n",
    "\n",
    "residual_spw0, mapping_0 = build_array(interpolated_data_dict, category=\"Residual\", spw=0)\n",
    "residual_spw1, mapping_1 = build_array(interpolated_data_dict, category=\"Residual\", spw=1)\n",
    "\n",
    "if np.all(np.array(mapping_0)[:, :-1] == np.array(mapping_1)[:, :-1]):\n",
    "    mapping = np.array(mapping_0)[:, :-1]\n",
    "    residual = np.concatenate((residual_spw0, residual_spw1), axis=1)\n",
    "else:\n",
    "    raise ValueError(\"Mappings for residual's spw0 and spw1 do not match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676c288",
   "metadata": {},
   "source": [
    "## Data filtering\n",
    "\n",
    "Three different filters are used in this work:\n",
    "- Threshold filter\n",
    "- Subtraction filter\n",
    "- Savitzky-Golay filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b12717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_processor import filter_data\n",
    "\n",
    "# Apply threhsold-filtering\n",
    "threshold_filtered_data = filter_data(raw_data, filter_type=\"sigma\", residual=residual)\n",
    "\n",
    "# Apply subtraction-filtering\n",
    "subtraction_filtered_data = filter_data(raw_data, filter_type=\"subtraction\", residual=residual)\n",
    "\n",
    "# Apply Savitzky-Golay filtering\n",
    "savgol_params = {'window_length':25, 'polyorder':3}\n",
    "savitzky_golay_filtered_data = filter_data(raw_data, filter_type=\"savgol\", **savgol_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022387e",
   "metadata": {},
   "source": [
    "## Red-shift correction\n",
    "\n",
    "The RedShiftCorrector class will identify a most-likely velocity for all sources when provided with the following data:\n",
    "- raw_data (array)\n",
    "- mapping (list)\n",
    "- residual (array)\n",
    "\n",
    "It will by default compare the velocities found for the raw-data, threshold-filtered data and subtraction-filtered data with a margin of +/- 5 km/s.\n",
    "\n",
    "The output variable will be a pandas DataFrame with the following columns:\n",
    "- Source\n",
    "- Core\n",
    "- None_velocity (velocity for the raw data)\n",
    "- threshold_velocity (velocity for the threshold-filtered data)\n",
    "- subtraction_velocity (velocity for the subtraction-filtered data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494ecbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering method: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shifting signals: 100%|██████████| 4396/4396 [24:09<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering method: subtraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shifting signals: 100%|██████████| 4396/4396 [24:18<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering method: threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javia\\OneDrive\\Desktop\\Programing\\MSc-Thesis\\src\\data\\data_processor.py:665: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  main_ax.set_ylabel(r'$\\Delta$V [km/s]')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "threshold is not recognised as an established filter type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m frequency = np.concatenate((frequency_spw0, frequency_spw1), axis=\u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Apply red-shift correction to the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m velocity_results = \u001b[43mRedShiftCorrector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrequency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Retrieve the indices for which a velocity was not found\u001b[39;00m\n\u001b[32m     16\u001b[39m no_velocity_indices = velocity_results[velocity_results.isna().any(axis=\u001b[32m1\u001b[39m)].index.to_list()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\javia\\OneDrive\\Desktop\\Programing\\MSc-Thesis\\src\\data\\data_processor.py:211\u001b[39m, in \u001b[36mRedShiftCorrector.fit\u001b[39m\u001b[34m(self, raw_data, mapping, residual, filtering, method)\u001b[39m\n\u001b[32m    209\u001b[39m     signal_data = raw_data\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     signal_data = \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m velocity_list = \u001b[38;5;28mself\u001b[39m._calculate_velocity(signal_data, method)\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# Define tag\\\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\javia\\OneDrive\\Desktop\\Programing\\MSc-Thesis\\src\\data\\data_processor.py:635\u001b[39m, in \u001b[36mfilter_data\u001b[39m\u001b[34m(data, filter_type, residual, **kwargs)\u001b[39m\n\u001b[32m    632\u001b[39m     filtered_data[filtered_data < \u001b[32m0\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not recognised as an established filter type\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_data\n",
      "\u001b[31mValueError\u001b[39m: threshold is not recognised as an established filter type"
     ]
    }
   ],
   "source": [
    "from src.data.data_processor import RedShiftCorrector\n",
    "from src.features.transformations import shift_signal\n",
    "\n",
    "# Retrieve the common frequency base\n",
    "region = list(interpolated_data_dict.keys())[0]\n",
    "core = list(interpolated_data_dict[region].keys())[0]\n",
    "\n",
    "frequency_spw0 = interpolated_data_dict[region][core][0]['Frequency']\n",
    "frequency_spw1 = interpolated_data_dict[region][core][1]['Frequency']\n",
    "frequency = np.concatenate((frequency_spw0, frequency_spw1), axis=0)\n",
    "\n",
    "# Apply red-shift correction to the data\n",
    "velocity_results = RedShiftCorrector(frequency=frequency).fit(raw_data, mapping, residual)\n",
    "\n",
    "# Retrieve the indices for which a velocity was not found\n",
    "no_velocity_indices = velocity_results[velocity_results.isna().any(axis=1)].index.to_list()\n",
    "\n",
    "# Shift the data\n",
    "shifted_signals = np.array([shift_signal(frequency, raw_data[i], velocity_results.loc[i].values) for i in range(raw_data.shape[0]) if i not in no_velocity_indices])\n",
    "shifted_residual = np.array([shift_signal(frequency, residual[i], velocity_results.loc[i].values) for i in range(raw_data.shape[0]) if i not in no_velocity_indices])\n",
    "threshold_shifted_signals = np.array([shift_signal(frequency, threshold_filtered_data[i], velocity_results.loc[i].values) for i in range(raw_data.shape[0]) if i not in no_velocity_indices])\n",
    "subtraction_shifted_signals = np.array([shift_signal(frequency, subtraction_filtered_data[i], velocity_results.loc[i].values) for i in range(raw_data.shape[0]) if i not in no_velocity_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2d678",
   "metadata": {},
   "source": [
    "The data should be saved for it to be used in other notebooks. Feel free to not run the next cell if you do not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data\n",
    "processed_data = {\n",
    "    'raw_data': raw_data,\n",
    "    'residual': residual,\n",
    "    'threshold_filtered_data': threshold_filtered_data,\n",
    "    'subtraction_filtered_data': subtraction_filtered_data,\n",
    "    'savitzky_golay_filtered_data': savitzky_golay_filtered_data,\n",
    "    'shifted_signals': shifted_signals,\n",
    "    'shifted_residual': shifted_residual,\n",
    "    'threshold_shifted_signals': threshold_shifted_signals,\n",
    "    'subtraction_shifted_signals': subtraction_shifted_signals,\n",
    "    'velocity_results': velocity_results,\n",
    "    'no_velocity_indices': no_velocity_indices,\n",
    "    'mapping': mapping,\n",
    "    'frequency': frequency\n",
    "}\n",
    "\n",
    "with open('Data/Processed/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca3ca7",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Two methods of dimensionality reduction were used in this study:\n",
    "- PCA\n",
    "- UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536c333",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5857c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for data in [shifted_signals, threshold_shifted_signals, subtraction_shifted_signals]:\n",
    "    # Reduce the data\n",
    "    pca = PCA(n_components=100).fit(data)\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.plot(range(1, 101), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "\n",
    "plt.legend([\"Raw data\", \"Threshold-filtered data\", \"Subtraction-filtered data\"])\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816db831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "pca_data = PCA(n_components=30).fit_transform(shifted_signals)\n",
    "np.save(\"Data/Processed/raw_pca_data.npy\", pca_data)\n",
    "\n",
    "# Threshold-filtered data\n",
    "pca_threshold_data = PCA(n_components=30).fit_transform(threshold_shifted_signals)\n",
    "np.save(\"Data/Processed/threshold_pca_data.npy\", pca_threshold_data)\n",
    "\n",
    "# Subtraction-filtered data\n",
    "pca_subtraction_data = PCA(n_components=30).fit_transform(subtraction_shifted_signals)\n",
    "np.save(\"Data/Processed/subtraction_pca_data.npy\", pca_subtraction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acb4e9",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7649d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "for data in [shifted_signals, threshold_shifted_signals, subtraction_shifted_signals]:\n",
    "    # Reduce the data\n",
    "    umap = UMAP(n_components=2).fit(data)\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.scatter(umap.embedding_[:,0], umap.embedding_[:,1], s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f45ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "umap_data = UMAP(n_components=2).fit_transform(shifted_signals)\n",
    "np.save(\"Data/Processed/raw_umap_data.npy\", umap_data)\n",
    "\n",
    "# Threshold-filtered data\n",
    "umap_threshold_data = UMAP(n_components=2).fit_transform(threshold_shifted_signals)\n",
    "np.save(\"Data/Processed/threshold_umap_data.npy\", umap_threshold_data)\n",
    "\n",
    "# Subtraction-filtered data\n",
    "umap_subtraction_data = UMAP(n_components=2).fit_transform(subtraction_shifted_signals)\n",
    "np.save(\"Data/Processed/subtraction_umap_data.npy\", umap_subtraction_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
